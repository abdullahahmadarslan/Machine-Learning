{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EqQfJSPno3PP",
        "outputId": "bfe2e878-693d-4045-df1e-ec77ec09a778"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens 1: ['welcome', 'to', 'great', 'learning', ',', 'now', 'start', 'learning']\n",
            "Tokens 2: ['learning', 'is', 'a', 'good', 'practice']\n",
            "Full Vocabulary (before filtering): ['welcome', 'to', 'great', 'learning', ',', 'now', 'start', 'is', 'a', 'good', 'practice']\n",
            "Filtered Vocabulary (final BoW vocab): ['welcome', 'great', 'learning', 'now', 'start', 'good', 'practice']\n",
            "Vector 1 (BoW for Sentence 1): [1, 1, 2, 1, 1, 0, 0]\n",
            "Vector 2 (BoW for Sentence 2): [0, 0, 1, 0, 0, 1, 1]\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Define a list of stopwords (commonly used words that don‚Äôt carry much meaning)\n",
        "stopwords = [\"to\", \"is\", \"a\"]\n",
        "\n",
        "# Step 2: Define a list of special characters to be filtered out\n",
        "special_char = [\",\", \":\", \" \", \";\", \".\", \"?\"]\n",
        "\n",
        "# Step 3: Write two sentences as input to the model (corpus of text)\n",
        "string1 = \"Welcome to Great Learning , Now start learning\"\n",
        "string2 = \"Learning is a good practice\"\n",
        "\n",
        "# Step 4: Convert both sentences to lowercase to ensure consistency\n",
        "string1 = string1.lower()\n",
        "string2 = string2.lower()\n",
        "\n",
        "# Step 5: Split (tokenize) the lowercase sentences into individual words\n",
        "tokens1 = string1.split()  # e.g., ['welcome', 'to', 'great', 'learning', ',', 'now', 'start', 'learning']\n",
        "tokens2 = string2.split()  # e.g., ['learning', 'is', 'a', 'good', 'practice']\n",
        "\n",
        "# Step 6: Print tokenized word lists for both sentences (for understanding)\n",
        "print(\"Tokens 1:\", tokens1)\n",
        "print(\"Tokens 2:\", tokens2)\n",
        "\n",
        "# Step 7: Combine both token lists and extract a unique vocabulary\n",
        "def unique(sequence):\n",
        "    \"\"\"\n",
        "    Returns a list of unique elements from the sequence, preserving the order.\n",
        "    \"\"\"\n",
        "    seen = set()\n",
        "    return [x for x in sequence if not (x in seen or seen.add(x))]\n",
        "    # x in seen: Checks if x has already been seen.\n",
        "       # If True, x is a duplicate.\n",
        "       # If False, it goes to the next part.\n",
        "    # seen.add(x): Adds x to the seen set\n",
        "       # Always returns None (which is False in boolean logic).\n",
        "\n",
        "vocab = unique(tokens1 + tokens2)\n",
        "print(\"Full Vocabulary (before filtering):\", vocab)\n",
        "\n",
        "# Step 8: Filter the vocabulary to exclude stopwords and special characters\n",
        "filtered_vocab = []\n",
        "for w in vocab:\n",
        "    if w not in stopwords and w not in special_char:\n",
        "        filtered_vocab.append(w)\n",
        "print(\"Filtered Vocabulary (final BoW vocab):\", filtered_vocab)\n",
        "\n",
        "# Step 9: Define the function to convert a list of tokens into a Bag of Words vector\n",
        "def vectorize(tokens):\n",
        "    \"\"\"\n",
        "    Converts a list of tokens into a BoW vector using filtered_vocab.\n",
        "    Each position in the vector corresponds to the frequency of a word from filtered_vocab.\n",
        "    \"\"\"\n",
        "    vector = []\n",
        "    for w in filtered_vocab:\n",
        "        vector.append(tokens.count(w)) # Counts how many times the word w appears in the list tokens.\n",
        "    return vector\n",
        "\n",
        "# Step 10: Convert each sentence into a Bag of Words vector\n",
        "vector1 = vectorize(tokens1)\n",
        "print(\"Vector 1 (BoW for Sentence 1):\", vector1)\n",
        "\n",
        "vector2 = vectorize(tokens2)\n",
        "print(\"Vector 2 (BoW for Sentence 2):\", vector2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üì¶ Bag of Words (BoW) vs üîç TF-IDF\n",
        "\n",
        "| Feature           | BoW (Bag of Words)                         | TF-IDF (Term Frequency-Inverse Document Frequency)           |\n",
        "|-------------------|---------------------------------------------|---------------------------------------------------------------|\n",
        "| What it captures  | Frequency of each word                     | Frequency adjusted by how rare the word is across all docs    |\n",
        "| Weights           | Raw count of words                         | Weighted score = TF √ó IDF                                     |\n",
        "| Common words      | Appear with high frequency and high score  | Down-weighted if they appear in many documents               |\n",
        "| Meaningful ranks  | No                                         | Yes ‚Äî rare but important words get higher weights             |\n",
        "| Normalization     | None by default                            | Includes normalization based on document frequency            |\n",
        "| Use-cases         | Simple tasks, prototyping, spam filters    | NLP tasks, document similarity, search engines, classifiers   |\n"
      ],
      "metadata": {
        "id": "RWHrFMHmrVfX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üß† Example\n",
        "\n",
        "Sentence: \"AI is the future and the future is AI\"\n",
        "Vocabulary: ['AI', 'is', 'the', 'future', 'and']\n",
        "\n",
        "BoW Vector: [2, 2, 2, 2, 1]\n",
        "TF-IDF Vector (example): [0.9, 0.5, 0.1, 0.9, 0.2]\n",
        "\n",
        "‚û° BoW gives equal importance to all frequent words.\n",
        "‚û° TF-IDF boosts important/rare terms like 'AI', reduces weight for common ones like 'is' and 'the'.\n"
      ],
      "metadata": {
        "id": "T_GwnLfysBBH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚úÖ When to Use What?\n",
        "\n",
        "üîπ Use BoW when:\n",
        "- You want a simple, fast, count-based model\n",
        "- The data is small\n",
        "- Word frequency itself is useful (e.g., spam detection)\n",
        "\n",
        "üî∏ Use TF-IDF when:\n",
        "- You need meaningful word importance\n",
        "- You want to reduce noise from common words\n",
        "- You are working on search, NLP classification, document similarity\n"
      ],
      "metadata": {
        "id": "tgfNKaDHsFEE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üß† Are BoW and TF-IDF Language Models?\n",
        "\n",
        "‚ùå No, BoW and TF-IDF are not language models.\n",
        "\n",
        "Reason:\n",
        "- They don‚Äôt consider word order\n",
        "- They don‚Äôt learn from data\n",
        "- They don‚Äôt calculate probability of word sequences\n",
        "\n",
        "‚úÖ Language Models:\n",
        "- Learn probability of word sequences\n",
        "- Understand context and structure\n",
        "- Examples: N-gram, RNNs, LSTMs, Transformers (BERT, GPT)\n",
        "\n",
        "| Technique   | Language Model? | Captures Order? | Learns from Data? |\n",
        "|-------------|------------------|------------------|--------------------|\n",
        "| BoW         | ‚ùå No             | ‚ùå No             | ‚ùå No              |\n",
        "| TF-IDF      | ‚ùå No             | ‚ùå No             | ‚ùå No              |\n",
        "| BERT/GPT    | ‚úÖ Yes            | ‚úÖ Yes            | ‚úÖ Yes             |\n"
      ],
      "metadata": {
        "id": "_U-1iGCGsUNe"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ehtha5GfsR3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y85BwFzAsQ-Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}